{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# numba 的基本用法\n",
    "\n",
    "## 使用 jit 加速 Python 低效的 for 语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "5.77 ms ± 229 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.66 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numba as nb\n",
    "import numpy as np\n",
    "\n",
    "def add1(x, c):\n",
    "    rs = [0.] * len(x)\n",
    "    for i, xx in enumerate(x):\n",
    "        rs[i] = xx + c\n",
    "    return rs\n",
    "\n",
    "def add2(x, c):\n",
    "    return [xx + c for xx in x]\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def add_with_jit(x, c):\n",
    "    rs = [0.] * len(x)\n",
    "    for i, xx in enumerate(x):\n",
    "        rs[i] = xx + c\n",
    "    return rs\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def wrong_add(x, c):\n",
    "    rs = [0] * len(x)\n",
    "    for i, xx in enumerate(x):\n",
    "        rs[i] = xx + c\n",
    "    return rs\n",
    "\n",
    "y = np.random.random(10**5).astype(np.float32)\n",
    "x = y.tolist()\n",
    "\n",
    "assert np.allclose(add1(x, 1), add2(x, 1), add_with_jit(x, 1))\n",
    "%timeit add1(x, 1)\n",
    "%timeit add2(x, 1)\n",
    "%timeit add_with_jit(x, 1)\n",
    "print(np.allclose(wrong_add(x, 1), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意：\n",
    "    + `numba`不支持 list comprehension，详情可参见[这里](https://github.com/numba/numba/issues/504)\n",
    "    + `jit`会在某种程度上“预编译”你的代码，这意味着它会在某种程度上固定住各个变量的数据类型；所以在`jit`下定义数组时，如果想要使用的是`float`数组的话，就不能像上述`wrong_add`里那样用`[0] * len(x)`定义、而应该在`0`后面加一个小数点：`[0.] * len(x)`\n",
    "    + `jit`能够加速的不限于`for`，但一般而言加速`for`会比较常见、效果也比较显著。我在我实现的`numpy`版本的卷积神经网络（`CNN`）中用了`jit`后、可以把代码加速 **20** 倍左右（具体代码可以参见[这里](https://github.com/carefree0910/MachineLearning/blob/master/NN/Basic/Layers.py#L9)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 vectorize 实现 numpy 的 Ufunc 功能\n",
    "\n",
    "### vectorize 的基本应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.33 ms ± 233 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "21.6 µs ± 209 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(y + 1, add_with_jit(x, 1))\n",
    "%timeit add_with_jit(x, 1)\n",
    "%timeit y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.6 µs ± 410 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "21.3 µs ± 236 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.vectorize(\"float32(float32, int32)\", nopython=True)\n",
    "def add_with_vec(yy, c):\n",
    "    return yy + c\n",
    "\n",
    "assert np.allclose(y + 1, add_with_vec(y, 1))\n",
    "%timeit add_with_vec(y, 1)\n",
    "%timeit y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.1 µs ± 2.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "21.3 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.vectorize(\"float32(float32, float32)\", nopython=True)\n",
    "def add_with_vec(yy, c):\n",
    "    return yy + c\n",
    "\n",
    "assert np.allclose(y + 1, add_with_vec(y, 1.))\n",
    "%timeit add_with_vec(y, 1.)\n",
    "%timeit y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.5 µs ± 339 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "50.8 µs ± 2.57 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "20.8 µs ± 155 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "20.7 µs ± 308 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.vectorize([\n",
    "    \"float32(float32, int32)\",\n",
    "    \"float32(float32, float32)\"\n",
    "], nopython=True)\n",
    "def add_with_vec(yy, c):\n",
    "    return yy + c\n",
    "\n",
    "assert np.allclose(y + 1, add_with_vec(y, 1), add_with_vec(y, 1.))\n",
    "%timeit add_with_vec(y, 1)\n",
    "%timeit add_with_vec(y, 1.)\n",
    "%timeit y + 1\n",
    "%timeit y + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorize 的“并行”版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.3 µs ± 2.43 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "20.7 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.vectorize(\"float32(float32, float32)\", target=\"parallel\", nopython=True)\n",
    "def add_with_vec(y, c):\n",
    "    return y + c\n",
    "\n",
    "assert np.allclose(y+1, add_with_vec(y,1.))\n",
    "%timeit add_with_vec(y, 1.)\n",
    "%timeit y + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意：并非所有版本的`numba`、`numpy`都能在上述`parallel`下获得如此明显的性能提升；事实上：\n",
    "    + 上述测试结果基于 Intel Distribution for Python\n",
    "    + 在默认的`Python3.6.1`环境下测试时，发现使用`parallel`是会更慢的……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.8 µs ± 3.45 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "101 µs ± 433 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "352 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.vectorize(\"float32(float32, float32, float32)\", target=\"parallel\", nopython=True)\n",
    "def clip_with_parallel(y, a, b):\n",
    "    if y < a:\n",
    "        return a\n",
    "    if y > b:\n",
    "        return b\n",
    "    return y\n",
    "\n",
    "@nb.vectorize(\"float32(float32, float32, float32)\", nopython=True)\n",
    "def clip(y, a, b):\n",
    "    if y < a:\n",
    "        return a\n",
    "    if y > b:\n",
    "        return b\n",
    "    return y\n",
    "\n",
    "assert np.allclose(np.clip(y, 0.1, 0.9), clip(y, 0.1, 0.9), clip_with_parallel(y, 0.1, 0.9))\n",
    "%timeit clip_with_parallel(y, 0.1, 0.9)\n",
    "%timeit clip(y, 0.1, 0.9)\n",
    "%timeit np.clip(y, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意：这个栗子中的性能提升就是实打实的了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总之，使用`parallel`时不能一概而论，还是要做些实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 jit(nogil=True) 实现高效并发（多线程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "7.1 ms ± 43.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "9.58 ms ± 57.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "7.29 ms ± 135 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.54 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def np_func(a, b):\n",
    "    return 1 / (a + np.exp(-b))\n",
    "\n",
    "@nb.jit('void(float32[:], float32[:], float32[:])', nopython=True, nogil=False)\n",
    "def kernel1(result, a, b):\n",
    "    for i in range(len(result)):\n",
    "        result[i] = 1 / (a[i] + math.exp(-b[i]))\n",
    "                \n",
    "@nb.jit('void(float32[:], float32[:], float32[:])', nopython=True, nogil=True)\n",
    "def kernel2(result, a, b):\n",
    "    for i in range(len(result)):\n",
    "        result[i] = 1 / (a[i] + math.exp(-b[i]))\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def make_single_task(kernel):\n",
    "    def func(length, *args):\n",
    "        result = np.empty(length, dtype=np.float32)\n",
    "        kernel(result, *args)\n",
    "        return result\n",
    "    return func\n",
    "\n",
    "def make_multi_task(kernel, n_thread):\n",
    "    def func(length, *args):\n",
    "        result = np.empty(length, dtype=np.float32)\n",
    "        args = (result,) + args\n",
    "        chunk_size = (length + n_thread - 1) // n_thread\n",
    "        chunks = [[arg[i*chunk_size:(i+1)*chunk_size] for i in range(n_thread)] for arg in args]\n",
    "        with ThreadPoolExecutor(max_workers=n_thread) as e:\n",
    "            for _ in e.map(kernel, *chunks):\n",
    "                pass\n",
    "        return result\n",
    "    return func\n",
    "\n",
    "length = 10 ** 6\n",
    "a = np.random.rand(length).astype(np.float32)\n",
    "b = np.random.rand(length).astype(np.float32)\n",
    "\n",
    "nb_func1 = make_single_task(kernel1)\n",
    "nb_func2 = make_multi_task(kernel1, 4)\n",
    "nb_func3 = make_single_task(kernel2)\n",
    "nb_func4 = make_multi_task(kernel2, 4)\n",
    "\n",
    "rs_np = np_func(a, b)\n",
    "rs_nb1 = nb_func1(length, a, b)\n",
    "rs_nb2 = nb_func2(length, a, b)\n",
    "rs_nb3 = nb_func3(length, a, b)\n",
    "rs_nb4 = nb_func4(length, a, b)\n",
    "assert np.allclose(rs_np, rs_nb1, rs_nb2, rs_nb3, rs_nb4)\n",
    "%timeit np_func(a, b)\n",
    "%timeit nb_func1(length, a, b)\n",
    "%timeit nb_func2(length, a, b)\n",
    "%timeit nb_func3(length, a, b)\n",
    "%timeit nb_func4(length, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意：一般来说，数据量越大、并发的效果越明显。反之，数据量小的时候，并发很有可能会降低性能"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:playground]",
   "language": "python",
   "name": "conda-env-playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}